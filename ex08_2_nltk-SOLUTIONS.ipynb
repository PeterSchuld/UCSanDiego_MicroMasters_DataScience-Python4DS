{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Exercise Notebook on Natural Language Processing: Solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` also provides access to a dataset of tweets from Twitter, it includes a set of tweets already classified as negative or positive.\n",
    "\n",
    "In this exercise notebook we would like to replicate the sentiment analysis classification performed on the movie reviews corpus on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Download and inspect the twitter_samples dataset\n",
    "\n",
    "First we want to download the dataset and inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/altintas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "nltk.download(\"twitter_samples\")\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First let's check the common `fileids` method of `nltk` corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The twitter_samples object has a `tokenized()` method that returns all tweets from a fileid already individually tokenized. Read its documentation and use it to find the number of positive and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "number_of_positive_tweets",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "number_of_positive_tweets = None\n",
    "### BEGIN SOLUTION\n",
    "number_of_positive_tweets = len(twitter_samples.tokenized(\"positive_tweets.json\"))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "number_of_negative_tweets",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "number_of_negative_tweets = None\n",
    "### BEGIN SOLUTION\n",
    "number_of_negative_tweets = len(twitter_samples.tokenized(\"negative_tweets.json\"))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "assert number_of_positive_tweets == 5000, \"Make sure you are counting the number of tweets, not the number of words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Build a bag-of-words model function\n",
    "\n",
    "As in the lecture, we can build a bag-of-words model to train our machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step we define a list of words that we want to filter out of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "build_bag_of_words_features_filtered",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def build_bag_of_words_features_filtered(words):\n",
    "    \"\"\"Build a bag of words model\"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return {\n",
    "        word:1 for word in words \\\n",
    "        if not word in useless_words}\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(build_bag_of_words_features_filtered([\"what\", \"the\", \"?\", \",\"]))==0, \"Make sure we are filtering out both stopwords and punctuation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: create a list of all words\n",
    "\n",
    "Before performing sentiment analysis, let's first inspect the dataset a little bit more by creating a list of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for dataset in [\"positive_tweets.json\", \"negative_tweets.json\"]:\n",
    "    for tweet in twitter_samples.tokenized(dataset):\n",
    "        words.extend(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the code above, see that it is a case of nested loop, for each dataset we are looping through each tweet. Also notice we are using `extend`, how does it differ from `append`? Try it on a simple case, or read the documentation or Google for it!\n",
    "\n",
    "Now let's filter out punctuation and stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "filtered_words",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "filtered_words = None\n",
    "### BEGIN SOLUTION\n",
    "filtered_words = [w for w in words if not w in useless_words]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to filter out `useless_words` as defined in the previous section, this will reduce the lenght of the dataset by more than a factor of 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY \n",
    "\n",
    "assert len(filtered_words) == 85637, \"Make sure that the filtering is applied correctly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: find the most common words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collection` package of the standard library contains a `Counter` class that is handy for counting frequencies of words in our list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also has a `most_common()` method to access the words with the higher count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "most_common_words",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "most_common_words = None\n",
    "### BEGIN SOLUTION\n",
    "most_common_words = counter.most_common()[:10]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert most_common_words[0][0] == \":(\", \"The most common word should be :(\"\n",
    "assert len(most_common_words) == 10, \"Make sure you are only getting the first 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Build the features for machine learning\n",
    "\n",
    "Using our `build_bag_of_words_features` function we can build separately the negative and positive features.\n",
    "\n",
    "The format of the positive features should be:\n",
    "\n",
    "    [\n",
    "        ( { \"here\":1, \"some\":1, \"words\":1 }, \"pos\" ),\n",
    "        ( { \"another\":1, \"tweet\":1}, \"pos\" )\n",
    "    ]\n",
    "    \n",
    "It is a list of tuples, the first element is a dictionary of the words with 1 if that word appears, the second the \"pos\" or \"neg\" string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "negative_features",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "negative_features = None\n",
    "### BEGIN SOLUTION\n",
    "negative_features = [\n",
    "    (build_bag_of_words_features_filtered(tweet), 'neg') \\\n",
    "    for tweet in twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "positive_features",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "positive_features = None\n",
    "### BEGIN SOLUTION\n",
    "positive_features = [\n",
    "    (build_bag_of_words_features_filtered(tweet), 'pos') \\\n",
    "    for tweet in twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#FollowFriday': 1,\n",
       " ':)': 1,\n",
       " '@France_Inte': 1,\n",
       " '@Milipol_Paris': 1,\n",
       " '@PKuchly57': 1,\n",
       " 'community': 1,\n",
       " 'engaged': 1,\n",
       " 'members': 1,\n",
       " 'top': 1,\n",
       " 'week': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert positive_features[0][1] == \"pos\", \"Make sure the feature is a list of tuples whose second element is pos or neg\"\n",
    "assert positive_features[0][0][\"engaged\"] == 1, \"Make sure that the first element of each tuple is a dictionary of words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Train a NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 80% of the data for training, the rest for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = int(len(positive_features) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(positive_features[:split]+negative_features[:split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy on the training and on the test sets, make sure to turn those into a percent value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "training_accuracy",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "training_accuracy = None\n",
    "### BEGIN SOLUTION\n",
    "training_accuracy = nltk.classify.util.accuracy(classifier, positive_features[:split]+negative_features[:split])*100\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "test_accuracy",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "test_accuracy = None\n",
    "### BEGIN SOLUTION\n",
    "test_accuracy = nltk.classify.util.accuracy(classifier, positive_features[split:]+negative_features[split:])*100\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the accuracy for the test is very high compared to the movie review dataset, check the most informative features below to understand why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      :( = 1                 neg : pos    =   2362.3 : 1.0\n",
      "                      :) = 1                 pos : neg    =   1139.0 : 1.0\n",
      "                     See = 1                 pos : neg    =     37.7 : 1.0\n",
      "                     TOO = 1                 neg : pos    =     36.3 : 1.0\n",
      "                  THANKS = 1                 neg : pos    =     35.0 : 1.0\n",
      "                    THAT = 1                 neg : pos    =     27.7 : 1.0\n",
      "                    miss = 1                 neg : pos    =     26.4 : 1.0\n",
      "                     sad = 1                 neg : pos    =     25.0 : 1.0\n",
      "                     x15 = 1                 neg : pos    =     23.7 : 1.0\n",
      "                   Thank = 1                 pos : neg    =     22.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
